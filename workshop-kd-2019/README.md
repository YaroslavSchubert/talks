
# References
* [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
* [Model compression via distillation and quantization](https://arxiv.org/abs/1802.05668)
* [Hardware-Software Codesign of Accurate, Multiplier-free Deep Neural Networks](https://arxiv.org/abs/1705.04288)
* [Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy](https://arxiv.org/abs/1711.05852)
* [N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning](https://arxiv.org/abs/1709.06030)
* [Faster gaze prediction with dense networks and Fisher pruning](https://arxiv.org/abs/1801.05787)




```
Dear colleagues,
We are pleased to invite you to attend the seminar “Knowledge Distillation”.


Knowledge Distillation is one of the way of model compression. When you train big and accurate model and that try to make it small and quick with same level of accuracy.

On the next seminar we will discuss this approach, do some coding and see how it can be used in Hokutake and PDC project.

Speaker: Kyryl Truskovskyi @kyryl

Presentation language: Russian.

Materials:
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model Compression.
Geoffrey Hinton, Oriol Vinyals and Jeff Dean. Distilling the Knowledge in a Neural Network.
Antonio Polino, Razvan Pascanu and Dan Alistarh. Model compression via distillation and quantization.

Hokchhay Tann, Soheil Hashemi, Iris Bahar and Sherief Reda. Hardware-Software Codesign of Accurate, Multiplier-free Deep Neural Networks.
Asit Mishra and Debbie Marr. Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy.
Anubhav Ashok, Nicholas Rhinehart, Fares Beainy and Kris M. Kitani. N2N learning: Network to Network Compression via Policy Gradient Reinforcement Learning.
Lucas Theis, Iryna Korshunova, Alykhan Tejani and Ferenc Huszár. Faster gaze prediction with dense networks and Fisher pruning.


```


https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322
https://www.youtube.com/watch?v=EK61htlw8hY
https://nervanasystems.github.io/distiller/knowledge_distillation/index.html


10 + 5  + 5 + 5 + 7
10 + 8  + 9 + 7 + 7